---
title: "Midterm"
author: "Mina Mehdinia"
output: rmdformats::material
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```

# problem

## I have a data set (p3.txt) to study the density of some material. I observe 3 different properties from lab (Prop1, Prop2, Prop3) and I guess these three properties may affect the density. Please fit a linear model and answer the following questions.

# (a) 
## If you fit a multiple linear model, analyze your result using what we have dis- cussed, including model fit, C.I. of parameters, Hypothesis testing of your model, and model diagnosis.

```{r}
data = read.table("p3.txt", header = TRUE)
```

```{r}
lmod <- lm(density ~ Prop1 + Prop2 + Prop3, data)
summary(lmod)
```
Our Ho: β(Prop1) = β(Prop1) = β(Prop1) = 0 and our H1: at least one is not equal to zero. From the summary of our model we can see that all the predictors are statistically significant at 5% level since their p-value is less than 0.05. Therefore we reject the null hypothesis which mean at least one of them is equal to zero, and all predictors are significantly individually in the model, and there is a relationship between density and 3 properties.in conclusion, we say that all the predictors should be included in our model.

If we take a look at the coefficient estimate for Prop1 and Prop2, we can see it's negative which means for Prop1, in one unit change in Prop1, density, will decrease by 1.0545620 when every other predictors are fixed. Same for Prop3, in one unit change in Prop3, the density will decrease by 0.0010386. For Prop2, we can say for one unit change in Prop2, the density will increase by 0.1036271 when every other predictors are fixed.

If we take a look at multiple R^2, the value is 0.9738 which means that model can explain 97% of the total variability, and because it is close to 1 we say it fits good to our model. 

```{r}
which.max(lmod$residuals)
```
Value 408 has the highest residual

```{r}
#Using F-test to see if predictors have a relationship to the response
lmodnull <- lm(density ~ 1, data)
anova(lmodnull,lmod)
```
From the result of anova we see that F-test is significant, hence the collection of these three predictors is significant to the response. 

```{r}
confint(lmod)
```
Form the CI result, we can see that none of those predicotrs at 95% level do not include zero, which mean significant of the predictor in the model.

```{r}
#removing intercept and compute RSS
lmodinterc <- lm(density ~ Prop1 + Prop2 + Prop3 -1, data)
summary(lmodinterc)
anova(lmodinterc, lmod)
```
After removing intercept from our model, and computing RSS, we can see that RSS is a bit higher in model without intercept. therefore it fits the model better.

```{r model_diagnosis}
#constant variance assumption
plot(fitted(lmod),residuals(lmod),xlab = "Fitted", ylab = "Residuals")
abline(h=0)
```
Since there is pattern in the above plot we would say that variance is not constant.


```{r}
#normality assumption
qqnorm(residuals(lmod), ylab = "Residuals")
qqline(residuals(lmod))
```
The plot shows that the observation approximately fit on the line, therefore we can say it satisfy the normality assumption.

```{r}
#large leverage
hatv <- hatvalues(lmod)
names <- row.names(data)
faraway::halfnorm(hatv, labs = names, ylab = "Leverages")
```
From above plot we can see that the value 42 and 343 have highest leverages which means they are different compared with other predictor values. 
```{r}
#outliers
stud <- rstudent(lmod)
stud[which.max(abs(stud))]

car::outlierTest(lmod)
```
We proof that the value 408 is an outliers. Also we showed that 408 has the highest residual.

```{r}
cookdist <- cooks.distance(lmod)
faraway::halfnorm(cookdist,3, labs = names, ylab = "Cook's Distance")

```
Form above graph we can see that the value 406,21, and 173 are candidate for influential points.

```{r}
#relation betwen predictor and response
car::avPlots(lmod)
```
From above plots we can see that there is strong negative linear relationship between variables Prop1 and density, and there is a strong positive linear relationship between variables Prop2 and density, and there is a weal negative linear relationship between Prop3 and density.

In conclusion, from the all tests we did above, the linear model is not appropriate since the variance is not constant, 


# (b)
## Predict the density at (0.27, 24, 2300) with its C.I. (for just single one point).
```{r}
x0 = data.frame(Prop1 =0.27, Prop2 = 24, Prop3 = 2300 )
predict(lmod, x0 , interval = "prediction", level = 0.95)
```
The predict density at (0.27, 24, 2300) is 2.918628, and has confidence interval (2.676178 , 3.161078) at 95% level.

# (c)
## Doing real experiment is very expensive for all of three properties. Assuming they are of equally likely expense. If I can only afford one property, which property do you suggest me do? Give me your reason.
```{r}
cor(data$density, data$Prop1)
cor(data$density, data$Prop2)
cor(data$density, data$Prop3)

lmodpro1 <- lm(density ~ Prop1, data)
lmodpro2 <- lm(density ~ Prop2, data)
lmodpro3 <- lm(density ~ Prop3, data)
summary(lmodpro1)
summary(lmodpro2)
summary(lmodpro3)

```
Form the above we can see that there is a high correlation between density and Property 2. If I wanna suggest one property, I should go with Prop2 as it will explain the largest variation in the density as compared to the other 2 properties. Also from the plot relation between predictor and response we saw that there is a strong positive linear relationship between variables Prop2 and density.
The other way to find out is to look at R^2 because R^2 gives a good indication how well our model perform. We use one predictor at a time and record the R^2 for each model. The R^2 for Prep1 is 31%, for Prep2 is 96% and for Prep3 is 9%. We can conclude that the second model is better than the other models.
The third thing that prove that we should go with the second model is the residual. The second model residual is smaller that the others wich means model has equally distributed around zero.
```{r}
plot(data$Prop1, data$density)
plot(data$Prop2, data$density)
plot(data$Prop3, data$density)

```
At the end I ploted each Properties vs our response, and we can see the second properties has the linear relation with our response.Therefore, the second model is better mdoel for doing real experiment.

# (d)
## What other ideas do you have from this data set?

With all models and graphs we can see that even if all predictions are statistically significant, and even if p-value is very small, we still can't decide if using linear model is appropriate or not. Sometimes when we see the full model with all prediction that is significant, it is hard to decide how to reduce the model, that is why we have to goes through multiple model (make it simpler and simpler) and check the summary of them to find out which model is best model, and to drop which predictor. For example, in this example after fitting 3 simple model, we saw that just one of them is good choice to suggest.
 